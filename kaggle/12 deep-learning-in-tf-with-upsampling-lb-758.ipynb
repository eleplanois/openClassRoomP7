{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "070aa916932e05e8bbfa399e76b6961f27a2b1d6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_curve, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "53a94fa84d9714c55eea3887471c05248b6338e0"
   },
   "source": [
    "![](https://www.homecredit.ph/files/copy-hc-logo.png)\n",
    "\n",
    "# DNN classifier in Tensorflow\n",
    "\n",
    "This kernel will build a DNN classifier for the Home Credit Default Risk competition. The challenge here (as always!) is to try and match the performance of the LightGBM/XGBoost classifiers which always seems tricky for NNs for this kind of problem.\n",
    "\n",
    "A lot of the feature engineering going into the model is from my previous kernel [here](https://www.kaggle.com/shep312/lightgbm-with-weighted-averages-dropout-771), so I will focus more on the NN graph development here.\n",
    "\n",
    "### Contents\n",
    "\n",
    "1. [Load and process data](#load)\n",
    "    1. [Check nulls](#nulls)\n",
    "    2. [Identify categoricals](#cats)\n",
    "    3. [Scaling](#scale)\n",
    "2. [Building the graph](#graph)\n",
    "3. [Training the NN](#train)\n",
    "4. [Analysis and submission](#submit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "934c044cff2a0f52e785b71d7537658021d96e7f"
   },
   "source": [
    "## 1. Load and process data <a name=\"load\"></a>\n",
    "\n",
    "First step is to load all the different .csvs into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7d394ce26aaaa01a39b5f880b79e4a7202cae424"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files:\n",
      "['application_test.csv', 'application_train.csv', 'bureau.csv', 'bureau_balance.csv', 'credit_card_balance.csv', 'HomeCredit_columns_description.csv', 'installments_payments.csv', 'POS_CASH_balance.csv', 'previous_application.csv', 'sample_submission.csv']\n",
      "Loading data sets...\n",
      "Data loaded.\n",
      "Main application training data set shape = (307511, 122)\n",
      "Main application test data set shape = (48744, 121)\n",
      "Positive target proportion = 0.08\n"
     ]
    }
   ],
   "source": [
    "input_dir = os.path.join(os.pardir, 'dataset')\n",
    "print('Input files:\\n{}'.format(os.listdir(input_dir)))\n",
    "print('Loading data sets...')\n",
    "\n",
    "sample_size = None\n",
    "app_train_df = pd.read_csv(os.path.join(input_dir, 'application_train.csv'), nrows=sample_size)\n",
    "app_test_df = pd.read_csv(os.path.join(input_dir, 'application_test.csv'), nrows=sample_size)\n",
    "bureau_df = pd.read_csv(os.path.join(input_dir, 'bureau.csv'), nrows=sample_size)\n",
    "bureau_balance_df = pd.read_csv(os.path.join(input_dir, 'bureau_balance.csv'), nrows=sample_size)\n",
    "credit_card_df = pd.read_csv(os.path.join(input_dir, 'credit_card_balance.csv'), nrows=sample_size)\n",
    "pos_cash_df = pd.read_csv(os.path.join(input_dir, 'POS_CASH_balance.csv'), nrows=sample_size)\n",
    "prev_app_df = pd.read_csv(os.path.join(input_dir, 'previous_application.csv'), nrows=sample_size)\n",
    "install_df = pd.read_csv(os.path.join(input_dir, 'installments_payments.csv'), nrows=sample_size)\n",
    "print('Data loaded.\\nMain application training data set shape = {}'.format(app_train_df.shape))\n",
    "print('Main application test data set shape = {}'.format(app_test_df.shape))\n",
    "print('Positive target proportion = {:.2f}'.format(app_train_df['TARGET'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "66a22e5a62550f5ae5638fe22119e8ac3f6ea0bc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_18</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>Revolving loans</td>\n",
       "      <td>M</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>F</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>Cash loans</td>\n",
       "      <td>M</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET NAME_CONTRACT_TYPE CODE_GENDER FLAG_OWN_CAR  \\\n",
       "0      100002       1         Cash loans           M            N   \n",
       "1      100003       0         Cash loans           F            N   \n",
       "2      100004       0    Revolving loans           M            Y   \n",
       "3      100006       0         Cash loans           F            N   \n",
       "4      100007       0         Cash loans           M            N   \n",
       "\n",
       "  FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0               Y             0          202500.0    406597.5      24700.5   \n",
       "1               N             0          270000.0   1293502.5      35698.5   \n",
       "2               Y             0           67500.0    135000.0       6750.0   \n",
       "3               Y             0          135000.0    312682.5      29686.5   \n",
       "4               Y             0          121500.0    513000.0      21865.5   \n",
       "\n",
       "   ...  FLAG_DOCUMENT_18 FLAG_DOCUMENT_19 FLAG_DOCUMENT_20 FLAG_DOCUMENT_21  \\\n",
       "0  ...                 0                0                0                0   \n",
       "1  ...                 0                0                0                0   \n",
       "2  ...                 0                0                0                0   \n",
       "3  ...                 0                0                0                0   \n",
       "4  ...                 0                0                0                0   \n",
       "\n",
       "  AMT_REQ_CREDIT_BUREAU_HOUR AMT_REQ_CREDIT_BUREAU_DAY  \\\n",
       "0                        0.0                       0.0   \n",
       "1                        0.0                       0.0   \n",
       "2                        0.0                       0.0   \n",
       "3                        NaN                       NaN   \n",
       "4                        0.0                       0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_WEEK  AMT_REQ_CREDIT_BUREAU_MON  \\\n",
       "0                         0.0                        0.0   \n",
       "1                         0.0                        0.0   \n",
       "2                         0.0                        0.0   \n",
       "3                         NaN                        NaN   \n",
       "4                         0.0                        0.0   \n",
       "\n",
       "   AMT_REQ_CREDIT_BUREAU_QRT  AMT_REQ_CREDIT_BUREAU_YEAR  \n",
       "0                        0.0                         1.0  \n",
       "1                        0.0                         0.0  \n",
       "2                        0.0                         0.0  \n",
       "3                        NaN                         NaN  \n",
       "4                        0.0                         0.0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "74e8a1fc611bf1c088b0ffbd6092e043bee49b0d",
    "code_folding": [
     0
    ],
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def feature_engineering(app_data, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                        pos_cash_df, prev_app_df, install_df):\n",
    "    \"\"\" \n",
    "    Process the input dataframes into a single one containing all the features. Requires\n",
    "    a lot of aggregating of the supplementary datasets such that they have an entry per\n",
    "    customer.\n",
    "    \n",
    "    Also, add any new features created from the existing ones\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Add new features\n",
    "    \n",
    "    # Amount loaned relative to salary\n",
    "    app_data['LOAN_INCOME_RATIO'] = app_data['AMT_CREDIT'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data['ANNUITY_INCOME_RATIO'] = app_data['AMT_ANNUITY'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data['ANNUITY LENGTH'] = app_data['AMT_CREDIT'] / app_data['AMT_ANNUITY']\n",
    "    \n",
    "    # # Aggregate and merge supplementary datasets\n",
    "    print('Combined train & test input shape before any merging  = {}'.format(app_data.shape))\n",
    "\n",
    "    # Previous applications\n",
    "    agg_funs = {'SK_ID_CURR': 'count', 'AMT_CREDIT': 'sum'}\n",
    "    prev_apps = prev_app_df.groupby('SK_ID_CURR').agg(agg_funs)\n",
    "    prev_apps.columns = ['PREV APP COUNT', 'TOTAL PREV LOAN AMT']\n",
    "    merged_df = app_data.merge(prev_apps, left_on='SK_ID_CURR', right_index=True, how='left')\n",
    "\n",
    "    # Average the rest of the previous app data\n",
    "    prev_apps_avg = prev_app_df.groupby('SK_ID_CURR').mean()\n",
    "    merged_df = merged_df.merge(prev_apps_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_PAVG'])\n",
    "    print('Shape after merging with previous apps num data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Previous app categorical features\n",
    "    prev_app_df, cat_feats, _ = process_dataframe(prev_app_df)\n",
    "    prev_apps_cat_avg = prev_app_df[cat_feats + ['SK_ID_CURR']].groupby('SK_ID_CURR')\\\n",
    "                             .agg({k: lambda x: str(x.mode().iloc[0]) for k in cat_feats})\n",
    "    merged_df = merged_df.merge(prev_apps_cat_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                            how='left', suffixes=['', '_BAVG'])\n",
    "    print('Shape after merging with previous apps cat data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit card data - numerical features\n",
    "    wm = lambda x: np.average(x, weights=-1/credit_card_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    credit_card_avgs = credit_card_df.groupby('SK_ID_CURR').agg(wm)   \n",
    "    merged_df = merged_df.merge(credit_card_avgs, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CCAVG'])\n",
    "    \n",
    "    # Credit card data - categorical features\n",
    "    most_recent_index = credit_card_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = credit_card_df.columns[credit_card_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(credit_card_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                       how='left', suffixes=['', '_CCAVG'])\n",
    "    print('Shape after merging with credit card data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit bureau data - numerical features\n",
    "    credit_bureau_avgs = bureau_df.groupby('SK_ID_CURR').mean()\n",
    "    merged_df = merged_df.merge(credit_bureau_avgs, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_BAVG'])\n",
    "    print('Shape after merging with credit bureau data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Bureau balance data\n",
    "    most_recent_index = bureau_balance_df.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].idxmax()\n",
    "    bureau_balance_df = bureau_balance_df.loc[most_recent_index, :]\n",
    "    merged_df = merged_df.merge(bureau_balance_df, left_on='SK_ID_BUREAU', right_on='SK_ID_BUREAU',\n",
    "                            how='left', suffixes=['', '_B_B'])\n",
    "    print('Shape after merging with bureau balance data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Pos cash data - weight values by recency when averaging\n",
    "    wm = lambda x: np.average(x, weights=-1/pos_cash_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    f = {'CNT_INSTALMENT': wm, 'CNT_INSTALMENT_FUTURE': wm, 'SK_DPD': wm, 'SK_DPD_DEF':wm}\n",
    "    cash_avg = pos_cash_df.groupby('SK_ID_CURR')['CNT_INSTALMENT','CNT_INSTALMENT_FUTURE',\n",
    "                                                 'SK_DPD', 'SK_DPD_DEF'].agg(f)\n",
    "    merged_df = merged_df.merge(cash_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CAVG'])\n",
    "    \n",
    "    # Pos cash data data - categorical features\n",
    "    most_recent_index = pos_cash_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = pos_cash_df.columns[pos_cash_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(pos_cash_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                       how='left', suffixes=['', '_CAVG'])\n",
    "    print('Shape after merging with pos cash data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Installments data\n",
    "    ins_avg = install_df.groupby('SK_ID_CURR').mean()\n",
    "    merged_df = merged_df.merge(ins_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_IAVG'])\n",
    "    print('Shape after merging with installments data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Add more value counts\n",
    "    merged_df = merged_df.merge(pd.DataFrame(bureau_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_BUREAU'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(credit_card_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_CRED_CARD'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(pos_cash_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_POS_CASH'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(install_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_INSTALL'])\n",
    "    print('Shape after merging with counts data = {}'.format(merged_df.shape))\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "304bcd7c390f46c33ba915e1d89fcd7906836a51",
    "code_folding": [
     0
    ],
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def process_dataframe(input_df, encoder_dict=None):\n",
    "    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n",
    "\n",
    "    # Label encode categoricals\n",
    "    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n",
    "    categorical_feats = categorical_feats\n",
    "    encoder_dict = {}\n",
    "    for feat in categorical_feats:\n",
    "        encoder = LabelEncoder()\n",
    "        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n",
    "        encoder_dict[feat] = encoder\n",
    "\n",
    "    return input_df, categorical_feats.tolist(), encoder_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf5a443cff51e779ce91cc143f1dc83ab478a206"
   },
   "source": [
    "Since they are in disparate .csv's next I need to merge them into a single use-able dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "90c1b5875135fdb93ceace0c1548444aea244b6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train & test input shape before any merging  = (356255, 125)\n",
      "Shape after merging with previous apps num data = (356255, 147)\n",
      "Shape after merging with previous apps cat data = (356255, 163)\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets into a single one for training\n",
    "len_train = len(app_train_df)\n",
    "app_both = pd.concat([app_train_df, app_test_df])\n",
    "merged_df = feature_engineering(app_both, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                                pos_cash_df, prev_app_df, install_df)\n",
    "\n",
    "# Separate metadata\n",
    "meta_cols = ['SK_ID_CURR', 'SK_ID_BUREAU', 'SK_ID_PREV']\n",
    "meta_df = merged_df[meta_cols]\n",
    "merged_df.drop(meta_cols, axis=1, inplace=True)\n",
    "\n",
    "# Process the data set.\n",
    "merged_df, categorical_feats, encoder_dict = process_dataframe(input_df=merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "56db4db61ec65e5b3391280646b6519bf4026635",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Capture other categorical features not as object data types:\n",
    "non_obj_categoricals = [\n",
    "    'FONDKAPREMONT_MODE',\n",
    "    'HOUR_APPR_PROCESS_START',\n",
    "    'HOUSETYPE_MODE',\n",
    "    'NAME_EDUCATION_TYPE',\n",
    "    'NAME_FAMILY_STATUS', \n",
    "    'NAME_HOUSING_TYPE',\n",
    "    'NAME_INCOME_TYPE', \n",
    "    'NAME_TYPE_SUITE', \n",
    "    'OCCUPATION_TYPE',\n",
    "    'ORGANIZATION_TYPE', \n",
    "    'WALLSMATERIAL_MODE',\n",
    "    'WEEKDAY_APPR_PROCESS_START', \n",
    "    'NAME_CONTRACT_TYPE_BAVG',\n",
    "    'WEEKDAY_APPR_PROCESS_START_BAVG',\n",
    "    'NAME_CASH_LOAN_PURPOSE', \n",
    "    'NAME_CONTRACT_STATUS', \n",
    "    'NAME_PAYMENT_TYPE',\n",
    "    'CODE_REJECT_REASON', \n",
    "    'NAME_TYPE_SUITE_BAVG', \n",
    "    'NAME_CLIENT_TYPE',\n",
    "    'NAME_GOODS_CATEGORY', \n",
    "    'NAME_PORTFOLIO', \n",
    "    'NAME_PRODUCT_TYPE',\n",
    "    'CHANNEL_TYPE', \n",
    "    'NAME_SELLER_INDUSTRY', \n",
    "    'NAME_YIELD_GROUP',\n",
    "    'PRODUCT_COMBINATION', \n",
    "    'NAME_CONTRACT_STATUS_CCAVG', \n",
    "    'STATUS',\n",
    "    'NAME_CONTRACT_STATUS_CAVG'\n",
    "]\n",
    "categorical_feats = categorical_feats + non_obj_categoricals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cca71199aee021128c0fddfcda0117c46fa9e5ab"
   },
   "source": [
    "Before I do any futher processing, extract the target variable for training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "318e1c65097fe0bc7efe25f3d8c38207ba055d74",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Extract target before scaling\n",
    "labels = merged_df.pop('TARGET')\n",
    "labels = labels[:len_train]\n",
    "\n",
    "# Reshape (one-hot)\n",
    "target = np.zeros([len(labels), len(np.unique(labels))])\n",
    "target[:, 0] = labels == 0\n",
    "target[:, 1] = labels == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "745a83755298c90f7474f6659892ca9f788c55cd"
   },
   "source": [
    "### 1.1 Check nulls <a name=\"nulls\"></a>\n",
    "\n",
    "The data set has a a few variables containing a lot of nulls. Drop any features that are over x% null, then fill with 0. This obviously isn't a great method and provides room for improvement later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "36b8d855b766e0c2b6cad3225e543d824452d35e"
   },
   "outputs": [],
   "source": [
    "null_counts = merged_df.isnull().sum()\n",
    "null_counts = null_counts[null_counts > 0]\n",
    "null_ratios = null_counts / len(merged_df)\n",
    "\n",
    "# Drop columns over x% null\n",
    "null_thresh = .8\n",
    "null_cols = null_ratios[null_ratios > null_thresh].index\n",
    "merged_df.drop(null_cols, axis=1, inplace=True)\n",
    "print('Columns dropped for being over {}% null:'.format(100*null_thresh))\n",
    "for col in null_cols:\n",
    "    print(col)\n",
    "    if col in categorical_feats:\n",
    "        categorical_feats.pop(col)\n",
    "    \n",
    "# Fill the rest with the mean (TODO: do something better!)\n",
    "# merged_df.fillna(merged_df.median(), inplace=True)\n",
    "merged_df.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "668207c75b71af15eb1cb79136550c3d66b54cd4"
   },
   "source": [
    "### 1.2 Identify categorical variables <a name=\"cats\"></a>\n",
    "\n",
    "Categorical variables will be important in this model - there are a lot of them and a few that have high cardinality. \n",
    "\n",
    "I will be creating embeddings to encode them for use in the model later, but for now just make a note of them and their positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "8ff78ae5f6a05adf5dbe0d9d08bdba88d7f4f609"
   },
   "outputs": [],
   "source": [
    "cat_feats_idx = np.array([merged_df.columns.get_loc(x) for x in categorical_feats])\n",
    "int_feats_idx = [merged_df.columns.get_loc(x) for x in non_obj_categoricals]\n",
    "cat_feat_lookup = pd.DataFrame({'feature': categorical_feats, 'column_index': cat_feats_idx})\n",
    "cat_feat_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "3677f92905602dc4af6fcb19362285af7f533488"
   },
   "outputs": [],
   "source": [
    "cont_feats_idx = np.array(\n",
    "    [merged_df.columns.get_loc(x) \n",
    "     for x in merged_df.columns[~merged_df.columns.isin(categorical_feats)]]\n",
    ")\n",
    "cont_feat_lookup = pd.DataFrame(\n",
    "    {'feature': merged_df.columns[~merged_df.columns.isin(categorical_feats)], \n",
    "     'column_index': cont_feats_idx}\n",
    ")\n",
    "cont_feat_lookup.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d06c4bacfb00fe86e7351919a5f433d2fdad1640"
   },
   "source": [
    "### 1.3 Scaling <a name=\"scale\"></a>\n",
    "\n",
    "Next data processing step is to scale the features so they don't get unfairly weighted against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "0291653a1997bb11e05a0c47fcc13315a5ff3d5a",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "final_col_names = merged_df.columns\n",
    "merged_df = merged_df.values\n",
    "merged_df[:, cont_feats_idx] = scaler.fit_transform(merged_df[:, cont_feats_idx])\n",
    "\n",
    "scaler_2 = MinMaxScaler(feature_range=(0, 1))\n",
    "merged_df[:, int_feats_idx] = scaler_2.fit_transform(merged_df[:, int_feats_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "98bb83b751d40b4d65c831462d53cddee8529bd6"
   },
   "source": [
    "## 2. Building the graph <a name=\"graph\"></a>\n",
    "\n",
    "Now the data is in decent shape, build the NN. \n",
    "\n",
    "First step, however, is to re-separate the competition train and test sets. I'll then further split the training set down to provide a hold out validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "dac6fe2603afe7c9d377ead5b68bfb749f5eaf13",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Re-separate into labelled and unlabelled\n",
    "train_df = merged_df[:len_train]\n",
    "predict_df = merged_df[len_train:]\n",
    "del merged_df, app_train_df, app_test_df, bureau_df, bureau_balance_df, credit_card_df, pos_cash_df, prev_app_df\n",
    "gc.collect()\n",
    "\n",
    "# Create a validation set to check training performance\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_df, target, test_size=0.1, random_state=2, stratify=target[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d342dd920ebdb095e33accf6713e304b04d9d3ae"
   },
   "source": [
    "Set the parameters for the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "115c68e6649a23df6a2b90fca021d6987243403d"
   },
   "outputs": [],
   "source": [
    "# Fixed graph parameters\n",
    "# EMBEDDING_SIZE = 3  # Use cardinality / 2 instead\n",
    "N_HIDDEN_1 = 80\n",
    "N_HIDDEN_2 = 80\n",
    "N_HIDDEN_3 = 40\n",
    "n_cont_inputs = X_train[:, cont_feats_idx].shape[1]\n",
    "n_classes = 2\n",
    "\n",
    "# Learning parameters\n",
    "LEARNING_RATE = 0.01\n",
    "N_EPOCHS = 30\n",
    "N_ITERATIONS = 400\n",
    "BATCH_SIZE = 250\n",
    "\n",
    "print('Number of continous features: ', n_cont_inputs)\n",
    "print('Number of categoricals pre-embedding: ', X_train[:, cat_feats_idx].shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b90afebf27658cdc2a00c8ee091f8134c772a57f"
   },
   "source": [
    "Graph itself. Note that there is an embedding step first where each categorical feature is embedded and attached to the continuous input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "576ef849c153b5d99538c9f2664029ec5bb94a3e",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def embed_and_attach(X, X_cat, cardinality):  \n",
    "    embedding = tf.Variable(tf.random_uniform([cardinality, cardinality // 2], -1.0, 1.0))\n",
    "    embedded_x = tf.nn.embedding_lookup(embedding, X_cat) \n",
    "    return tf.concat([embedded_x, X], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "bcc849802453459b7775a0042b32639200b7f50e",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Define placeholders for the categorical varaibles\n",
    "cat_placeholders, cat_cardinalities = [], []\n",
    "for idx in cat_feats_idx:\n",
    "    exec('X_cat_{} = tf.placeholder(tf.int32, shape=(None, ), name=\\'X_cat_{}\\')'.format(idx, idx))\n",
    "    exec('cat_placeholders.append(X_cat_{})'.format(idx))\n",
    "    cat_cardinalities.append(len(np.unique(np.concatenate([train_df[:, idx], \n",
    "                                                           predict_df[:, idx]], axis=0))))\n",
    "    \n",
    "# Other placeholders\n",
    "X_cont = tf.placeholder(tf.float32, shape=(None, n_cont_inputs), name='X_cont')\n",
    "y = tf.placeholder(tf.int32, shape=(None, n_classes), name='labels')\n",
    "train_mode = tf.placeholder(tf.bool)\n",
    "\n",
    "# Add embeddings to input\n",
    "X = tf.identity(X_cont)\n",
    "for feat, card in zip(cat_placeholders, cat_cardinalities):\n",
    "    X = embed_and_attach(X, feat, card)\n",
    "\n",
    "# Define the network layers. \n",
    "# Overfitting is a challenge so add L2 regularisation to weights in 1st layer & \n",
    "# a couple of dropout layers\n",
    "with tf.name_scope('dnn'):\n",
    "    hidden_layer_1 = tf.layers.dense(inputs=X,\n",
    "                                     units=N_HIDDEN_1,\n",
    "                                     name='first_hidden_layer',\n",
    "                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.3))\n",
    "    hidden_layer_1 = tf.layers.batch_normalization(hidden_layer_1, training=train_mode)\n",
    "    hidden_layer_1 = tf.nn.relu(hidden_layer_1)\n",
    "    \n",
    "    drop_layer_1 = tf.layers.dropout(inputs=hidden_layer_1, \n",
    "                                     rate=0.4, \n",
    "                                     name='first_dropout_layer',\n",
    "                                     training=train_mode)\n",
    "\n",
    "    hidden_layer_2 = tf.layers.dense(inputs=drop_layer_1,\n",
    "                                     units=N_HIDDEN_2,\n",
    "                                     name='second_hidden_layer',\n",
    "                                     kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=0.1))\n",
    "    hidden_layer_2 = tf.layers.batch_normalization(hidden_layer_2, training=train_mode)\n",
    "    hidden_layer_2 = tf.nn.relu(hidden_layer_2)\n",
    "                                     \n",
    "    drop_layer_2 = tf.layers.dropout(inputs=hidden_layer_2, \n",
    "                                     rate=0.2, \n",
    "                                     name='second_dropout_layer',\n",
    "                                     training=train_mode)\n",
    "\n",
    "    hidden_layer_3 = tf.layers.dense(inputs=drop_layer_2,\n",
    "                                     units=N_HIDDEN_3,\n",
    "                                     name='third_hidden_layer')\n",
    "    hidden_layer_3 = tf.layers.batch_normalization(hidden_layer_3, training=train_mode)\n",
    "    hidden_layer_3 = tf.nn.relu(hidden_layer_3)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=hidden_layer_3,\n",
    "                             units=n_classes,\n",
    "                             name='outputs')\n",
    "\n",
    "# Define the loss function for training as cross entropy\n",
    "with tf.name_scope('loss'):\n",
    "    xent = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xent, name='loss')\n",
    "\n",
    "# Define the optimiser\n",
    "with tf.name_scope('train'):\n",
    "    optimiser = tf.train.AdamOptimizer()  # AdagradOptimizer(learning_rate=LEARNING_RATE)\n",
    "    train_step = optimiser.minimize(loss)\n",
    "\n",
    "# Output the class probabilities to I can get the AUC\n",
    "with tf.name_scope('eval'):\n",
    "    predict = tf.argmax(logits, axis=1, name='class_predictions')\n",
    "    predict_proba = tf.nn.softmax(logits, name='probability_predictions')\n",
    "\n",
    "# Initialisation node and saver\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c49702ed1cd77a3422ae08dab65de4bafcdff932"
   },
   "source": [
    "## 3. Training the NN <a name=\"train\"></a>\n",
    "\n",
    "Time to train the network. We know that only 8% of the targets are positive, so I will be upsampling positives for the gradient descent batches. I'm going to go with even representation in the training batches, but this isn't necessarily going to get the best score so may need optimising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "4131d595c68fbcea0e99226b5e64c36a2f8c513f",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_feed_dict(cat_feats_idx, cat_placeholders, cont_feats_idx, batch_X, batch_y=None,\n",
    "                 training=False):\n",
    "    \"\"\" Return a feed dict for the graph including all the categorical features\n",
    "    to embed \"\"\"\n",
    "    \n",
    "    # Continuous X features and the labels if training run\n",
    "    feed_dict = {X_cont: batch_X[:, cont_feats_idx]}\n",
    "    if batch_y is not None:\n",
    "        feed_dict[y] = batch_y\n",
    "        \n",
    "    # Loop through the categorical features to provide values for the placeholders\n",
    "    for idx, tensor in zip(cat_feats_idx, cat_placeholders):\n",
    "        feed_dict[tensor] = batch_X[:, idx].reshape(-1, ).astype(int)\n",
    "        \n",
    "    # Training mode or not\n",
    "    feed_dict[train_mode] = training\n",
    "        \n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "bb77df2430b5ece84e3347323a74b54157ea8946"
   },
   "outputs": [],
   "source": [
    "train_auc, valid_auc = [], []\n",
    "n_rounds_not_improved = 0\n",
    "early_stopping_epochs = 2\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    init.run()\n",
    "\n",
    "    # Begin epoch loop\n",
    "    print('Training for {} iterations over {} epochs with batchsize {} ...'\n",
    "          .format(N_ITERATIONS, N_EPOCHS, BATCH_SIZE))\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        \n",
    "        # Iteration loop\n",
    "        for iteration in range(N_ITERATIONS):\n",
    "\n",
    "            # Get random selection of data for batch GD. Upsample positive classes to make it\n",
    "            # balanced in the training batch\n",
    "            pos_ratio = 0.5\n",
    "            pos_idx = np.random.choice(np.where(y_train[:, 1] == 1)[0], \n",
    "                                       size=int(np.round(BATCH_SIZE*pos_ratio)))\n",
    "            neg_idx = np.random.choice(np.where(y_train[:, 1] == 0)[0], \n",
    "                                       size=int(np.round(BATCH_SIZE*(1-pos_ratio))))\n",
    "            idx = np.concatenate([pos_idx, neg_idx])\n",
    "            \n",
    "            # Run training\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "            sess.run([train_step, extra_update_ops], \n",
    "                     feed_dict=get_feed_dict(cat_feats_idx, cat_placeholders, cont_feats_idx, \n",
    "                                             X_train[idx, :], y_train[idx, :], 1))\n",
    "\n",
    "        # Check on the AUC\n",
    "        y_pred_train, y_prob_train = sess.run(\n",
    "            [predict, predict_proba], feed_dict=get_feed_dict(\n",
    "                cat_feats_idx, cat_placeholders, cont_feats_idx, X_train, y_train, False))\n",
    "        train_auc.append(roc_auc_score(y_train[:, 1], y_prob_train[:, 1]))\n",
    "        \n",
    "        y_pred_val, y_prob_val = sess.run(\n",
    "            [predict, predict_proba], feed_dict=get_feed_dict(\n",
    "                cat_feats_idx, cat_placeholders, cont_feats_idx, X_valid, y_valid, False))\n",
    "        valid_auc.append(roc_auc_score(y_valid[:, 1], y_prob_val[:, 1]))\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch > 1:\n",
    "            best_epoch_so_far = np.argmax(valid_auc[:-1])\n",
    "            if valid_auc[epoch] <= valid_auc[best_epoch_so_far]:\n",
    "                n_rounds_not_improved += 1\n",
    "            else:\n",
    "                n_rounds_not_improved = 0       \n",
    "            if n_rounds_not_improved > early_stopping_epochs:\n",
    "                print('Early stopping due to no improvement after {} epochs.'\n",
    "                      .format(early_stopping_epochs))\n",
    "                break\n",
    "        print('Epoch = {}, Train AUC = {:.8f}, Valid AUC = {:.8f}'\n",
    "              .format(epoch, train_auc[epoch], valid_auc[epoch]))\n",
    "\n",
    "    # Once trained, make predictions\n",
    "    print('Training complete.')\n",
    "    y_prob = sess.run(predict_proba, feed_dict=get_feed_dict(\n",
    "        cat_feats_idx, cat_placeholders, cont_feats_idx, predict_df, None, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0dbff3033889c123c2c8358d8b11be0bfff5465"
   },
   "source": [
    "So an OK performance, but not matching the gradient boosted results yet. There's a lot of avenues to improve, however, including but not limited to:\n",
    "\n",
    "- **Optimising number of nodes / width**: Currently I've only used a small number of nodes in the hidden layers, limiting the complexity of the model. More nodes should lead to better performance, but I found that it started to overfit. There is probably a way to regularise the network to allow it to get more complex\n",
    "- **Number of layers**: Currently at 3 hidden layers deep, could be a better number\n",
    "- **Upsampling of positives**: Reduction in the upsampling of postive samples could be tuned\n",
    "- **Other hyperparameters**: Learning rate, batch size etc. could be tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85dbfc50999caa30e81840a310025bdee2dd0a45"
   },
   "source": [
    "## 4. Analysis and Submission <a name=\"submit\"></a>\n",
    "\n",
    "Lets have a look at some summarising plots, then submit the results. \n",
    "\n",
    "Training curves & the ROC curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "66366ac0f41005fc07a91361eb18661f3c527347",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax, ax1) = plt.subplots(1, 2, figsize=[14, 5])\n",
    "ax.plot(np.arange(len(train_auc)), train_auc, label='Train')\n",
    "ax.plot(np.arange(len(valid_auc)), valid_auc, label='Valid')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('AUC')\n",
    "ax.set_title('Training performance')\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_valid[:, 1], y_prob_val[:, 1])\n",
    "ax1.plot(fpr, tpr, label='ROC curve (area = {:.2f})'.format(valid_auc[epoch]))\n",
    "ax1.plot([0, 1], [0, 1], linestyle='--')\n",
    "ax1.set_xlim([0.0, 1.0])\n",
    "ax1.set_ylim([0.0, 1.05])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC Curve')\n",
    "\n",
    "for a in [ax, ax1]:\n",
    "    a.spines['top'].set_visible(False)\n",
    "    a.spines['right'].set_visible(False)\n",
    "    a.legend(frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c0781b2f92972c3eb4434a12733b13ce5f89dd45"
   },
   "source": [
    "...and a precision-recall curve and the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e9de5f51cd64ff71074161e68b387233c8aeffd7",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax, ax1) = plt.subplots(1, 2, figsize=[14, 5])\n",
    "\n",
    "# Precision recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_valid[:, 1], y_prob_val[:, 1])\n",
    "ax.step(recall, precision, color='b', alpha=0.2, where='post')\n",
    "ax.fill_between(recall, precision, step='post', alpha=0.2, color='b')\n",
    "ax.set_xlabel('Recall')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_ylim([0.0, 1.05])\n",
    "ax.set_xlim([0.0, 1.0])\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.set_title('Precision - recall curve')\n",
    "\n",
    "# Confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_valid[:, 1], np.argmax(y_prob_val, axis=1))\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "heatmap = sns.heatmap(cnf_matrix, annot=True, fmt='d', ax=ax1, cmap=cmap, center=0)\n",
    "ax1.set_title('Confusion matrix heatmap')\n",
    "ax1.set_ylabel('True label')\n",
    "ax1.set_xlabel('Predicted label')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e37c6b930593dbd372282fe5af551fe5c4b1650",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "out_df = pd.DataFrame({'SK_ID_CURR': meta_df['SK_ID_CURR'][len_train:], 'TARGET': y_prob[:, 1]})\n",
    "out_df.to_csv('nn_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
